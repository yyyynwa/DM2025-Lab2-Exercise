{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Student Information**\n",
    "Name: 余雅韻\n",
    "\n",
    "Student ID: 114577002\n",
    "\n",
    "GitHub ID: yyyynwa\n",
    "\n",
    "Kaggle name: Yyy (Yyyun2001)\n",
    "\n",
    "Kaggle private scoreboard snapshot: \n",
    "\n",
    "![pic_ranking.png](./pics/pic_ranking.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "# **Project Report**\n",
    "\n",
    "## 1. Model Development (10 pts Required)\n",
    "\n",
    "### 1.1 Preprocessing Steps\n",
    "\n",
    "| **Classical Model:**                       | **RoBERTa Model:**                                  |\n",
    "|--------------------------------------------|-----------------------------------------------------|\n",
    "| Aggressive cleaning for feature extraction | Minimal preprocessing to preserve emotional signals |\n",
    "| Lowercase conversion                       | Only removed URLs and normalized whitespace         |\n",
    "| Contraction expansion (n't → not)          | Kept capitalization (CAPS indicate emotion)         |\n",
    "| URL and @mention removal                   | Preserved punctuation (!!!, ??? show intensity)     |\n",
    "| Hashtag symbol removal (keep words)        | Kept emojis (strong emotion indicators)             |\n",
    "| Whitespace normalization                   |                                                     |\n",
    "\n",
    "\n",
    "### 1.2 Feature Engineering Steps\n",
    "\n",
    "**Classical Model (Manual Features):**\n",
    "\n",
    "1.  TF-IDF Features\n",
    "\n",
    "* Word-level unigrams/bigrams\n",
    "* Character n-grams 2-4 \n",
    "* Captures misspellings and writing style\n",
    "\n",
    "\n",
    "2. PMI-Based Emotion Lexicons\n",
    "\n",
    "* Learned emotion-specific vocabularies from training data\n",
    "* Top 300 words per emotion with highest PMI scores\n",
    "* Captures domain-specific emotion expressions\n",
    "\n",
    "\n",
    "3. Disgust-Specific Features \n",
    "\n",
    "* Custom lexicon of 40+ disgust-related words\n",
    "* Count features: total, binary indicator, intensity (clipped)\n",
    "* For improving disgust detection (hardest class)\n",
    "\n",
    "\n",
    "4. SVD Dimensionality Reduction\n",
    "\n",
    "* Applied to TF-IDF for dense semantic representation\n",
    "* Explained 30-40% variance\n",
    "* Reduces noise and overfitting\n",
    "\n",
    "\n",
    "5. VADER Sentiment Scores\n",
    "\n",
    "* Positive, negative, neutral, compound scores\n",
    "* Pre-built sentiment lexicon\n",
    "* Standardized using StandardScaler\n",
    "\n",
    "\n",
    "**RoBERTa Model (Learned Features):**\n",
    "\n",
    "1. No manual feature engineering required\n",
    "2. Tokenization using subword units (50,265 vocabulary)\n",
    "3. Captures semantic relationships, context, long-range dependencies\n",
    "\n",
    "### 1.3 Explanation of Your Model\n",
    "\n",
    "| **Classical ML Pipeline**                                              | **RoBERTa Transformer**                                  | **Ensemble Strategy**                                         |\n",
    "|------------------------------------------------------------------------|----------------------------------------------------------|---------------------------------------------------------------|\n",
    "| **Model: LinearSVC with Probability Calibration**                      | **Architecture:**                                        | **Combination Method: Weighted probability averaging**        |\n",
    "| Fast and effective for high-dimensional text data                      | Pre-trained RoBERTa-base model                           | RoBERTa: 65% weight (strong at anger, fear, surprise)         |\n",
    "| Linear decision boundaries                                             | 12 transformer encoder layers                            | Classical: 35% weight (strong at disgust)                     |\n",
    "| Regularization: C=0.2 (prevent overfitting)                            | 768 hidden dimensions                                    | **Weight Optimization:**                                      |\n",
    "| dual=False (faster for n_samples > n_features)                         | 12 attention heads per layer                             | Grid search over weights [0.50, 0.55, 0.60, 0.65, 0.70, 0.75] |\n",
    "| **Class Imbalance Handling:**                                          | Fine-tuned all layers for emotion classification         | Evaluated on out-of-fold predictions                          |\n",
    "| Custom class weights (balanced + 2.5x boost for disgust)               | **Training Strategy:**                                   | Selected weights maximizing macro F1                          |\n",
    "| Addresses severe class imbalance in training data                      | 3-fold Stratified K-Fold cross-validation                | **Final Performance:**                                        |\n",
    "| Critical for minority class performance                                | 3 epochs per fold / Batch size: 16                       | Training F1: 0.6700                                           |\n",
    "| **Probability Calibration:**                                           | Learning rate: 2e-5 with linear warmup                   | Public Leaderboard F1: 0.6889                                 |\n",
    "| CalibratedClassifierCV with sigmoid method                             | AdamW optimizer                                          |                                                               |\n",
    "| 3-fold cross-validation                                                | Gradient clipping (max_norm=1.0)                         |                                                               |\n",
    "| Converts SVM scores to probabilities for ensemble                      | Class weights: 2.0x boost for disgust                    |                                                               |\n",
    "| **Performance:**                                                       | **Performance:**                                         |                                                               |\n",
    "| Training F1: ~0.5800                                                   | Training F1: ~0.5600                                     |                                                               |\n",
    "| Strong at: disgust detection (21.2% accuracy vs 5-10% in other models) | Strong at: anger (64.4%), fear (58.1%), surprise (54.8%) |                                                               |\n",
    "|                                                                        | Weak at: disgust (10-15%)                                |                                                               |\n",
    "\n",
    "**Why Ensemble Works:**\n",
    "\n",
    "1. Complementary strengths: RoBERTa captures context, Classical captures specific patterns\n",
    "2. Diversity: Different feature representations\n",
    "3. Reduces overfitting: Averages out individual model biases\n",
    "4. Boosts weak classes: Classical compensates for RoBERTa's poor disgust performance\n",
    "---\n",
    "\n",
    "## 2. Bonus Section (5 pts Optional)\n",
    "\n",
    "### 2.1 Mention Different Things You Tried\n",
    "\n",
    "| **Successful Experiments**                                                   | **Unsuccessful Experiments**                                           |\n",
    "|------------------------------------------------------------------------------|------------------------------------------------------------------------|\n",
    "| PMI-based emotion lexicons - Added emotion-specific vocabulary (+0.02 F1)    | Threshold optimization - Severe overfitting (train 0.72 → val 0.55)    |\n",
    "| Custom disgust features - Dramatically improved disgust detection (+0.04 F1) | Excessive feature engineering - More features led to worse performance |\n",
    "| Character n-grams - Captured misspellings and style (+0.01 F1)               | Single model optimization - Hit ceiling around 0.58-0.60 F1            |\n",
    "| SVD dimensionality reduction - Reduced noise, improved generalization        | Data augmentation - Didn't help transformers significantly             |\n",
    "| Class weight tuning - Essential for minority classes                         | Deeper models - RoBERTa-large was too slow, minimal gain               |\n",
    "| Ensemble combination - Best improvement (+0.09 F1)                           |                                                                        |\n",
    "| K-fold cross-validation - Robust performance estimates                       |                                                                        |\n",
    "\n",
    "![Submission records the things I've tried](./pics/Tried2.png)\n",
    "![Submission records the things I've tried](./pics/Tried1.png)\n",
    "\n",
    "### 2.2 Mention Insights You Gained\n",
    "\n",
    "**Challenges in This Task :**\n",
    "1. Disgust detection: Consistently worst-performing class across all models\n",
    "2. Class imbalance: Required multiple strategies (weights, features, ensemble)\n",
    "3. Overfitting: High-dimensional features led to train/val gap\n",
    "4. Confusion between similar emotions: Joy vs surprise, fear vs sadness\n",
    "\n",
    "**Key Insights & Learnings**\n",
    "1. Class Imbalance is Critical\n",
    "\n",
    "* Disgust was severely underrepresented and hardest to detect\n",
    "* Required specialized features + class weighting\n",
    "* Single approach wasn't enough --> needed both targeted features and ensemble\n",
    "\n",
    "2. More Features ≠ Better Performance\n",
    "\n",
    "* Initial attempts with excessive features led to overfitting\n",
    "* Simpler models MAY be generalized better\n",
    "* Feature selection and regularization were crucial\n",
    "\n",
    "3. Ensemble Diversity is Key\n",
    "\n",
    "* RoBERTa and Classical models made different types of errors\n",
    "* Combining them captured more patterns than either alone\n",
    "\n",
    "4. Cross-Validation Prevents Overfitting\n",
    "\n",
    "* Used 3-fold stratified CV for both models\n",
    "* Out-of-fold predictions for unbiased ensemble training\n",
    "* Gap between train/val indicated overfitting\n",
    "\n",
    "5. Domain Knowledge Matters\n",
    "\n",
    "* Custom disgust lexicon significantly improved performance\n",
    "* Understanding social media language patterns helped preprocessing\n",
    "* PMI-based lexicons captured emotion-specific vocabulary\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**`From here on starts the code section for the competition.`**\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Competition Code**\n",
    "\n",
    "## 1. Preprocessing Steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Add the code related to the preprocessing steps in cells inside this section\n",
    "\n",
    "def clean_text(text):\n",
    "    \"\"\"\n",
    "    Clean and normalize text data.\n",
    "    \n",
    "    Preprocessing steps:\n",
    "    1. Convert to lowercase for consistency\n",
    "    2. Expand contractions (n't -> not) to preserve negation\n",
    "    3. Remove URLs (not informative for emotion)\n",
    "    4. Remove @mentions (privacy and generalization)\n",
    "    5. Remove hashtag symbols but keep content\n",
    "    6. Normalize whitespace\n",
    "    \n",
    "    Args:\n",
    "        text: Raw text string\n",
    "    \n",
    "    Returns:\n",
    "        Cleaned text string\n",
    "    \"\"\"\n",
    "    text = str(text).lower()\n",
    "    \n",
    "    # Expand contractions to preserve negation\n",
    "    text = re.sub(r\"n't\\b\", \" not\", text)\n",
    "    \n",
    "    # Remove URLs\n",
    "    text = re.sub(r'http\\S+|www\\S+', '', text)\n",
    "    \n",
    "    # Remove @mentions\n",
    "    text = re.sub(r'@\\w+', '', text)\n",
    "    \n",
    "    # Remove hashtag symbol but keep the word\n",
    "    text = re.sub(r'#(\\w+)', r'\\1', text)\n",
    "    \n",
    "    # Normalize whitespace\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    \n",
    "    return text\n",
    "\n",
    "# Apply preprocessing\n",
    "train_df['clean_text'] = train_df['text'].apply(clean_text)\n",
    "test_df['clean_text'] = test_df['text'].apply(clean_text)\n",
    "\n",
    "print(\"✓ Applied text cleaning\")\n",
    "print(\"\\nExample transformation:\")\n",
    "print(f\"Original: {train_df['text'].iloc[0][:100]}\")\n",
    "print(f\"Cleaned:  {train_df['clean_text'].iloc[0][:100]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Feature Engineering Steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Add the code related to the feature engineering steps in cells inside this section\n",
    "\n",
    "## Domain-Specific Features (Disgust Detection) ##\n",
    "\n",
    "# Define disgust-related vocabulary based on domain knowledge\n",
    "DISGUST_WORDS = {\n",
    "    # Core disgust terms\n",
    "    'disgusting', 'gross', 'nasty', 'revolting', 'repulsive', 'vile', \n",
    "    'filthy', 'foul', 'horrible', 'yuck', 'ew', 'eww', 'ugh',\n",
    "    \n",
    "    # Bodily reactions\n",
    "    'sick', 'puke', 'vomit', 'nauseous', 'gag', 'rotten', 'stink',\n",
    "    'smell', 'smells', 'stinks', 'reeks',\n",
    "    \n",
    "    # Visual disgust\n",
    "    'ugly', 'hideous', 'grotesque', 'repugnant',\n",
    "    \n",
    "    # Contamination\n",
    "    'dirty', 'filth', 'contaminated', 'infection', 'disease',\n",
    "    'germs', 'bacteria', 'mold', 'decay',\n",
    "    \n",
    "    # Emotional responses\n",
    "    'disgusted', 'appalled', 'repulsed', 'sickening',\n",
    "    \n",
    "    # Internet slang\n",
    "    'cringe', 'cringy', 'cringeworthy'\n",
    "}\n",
    "\n",
    "\n",
    "#    Extract features specifically designed to detect disgust emotion\n",
    "def extract_disgust_features(df):\n",
    "    feats = pd.DataFrame()\n",
    "    texts_lower = df['clean_text'].str.lower()\n",
    "    \n",
    "    # Count disgust words using word boundary matching\n",
    "    feats['disgust_word_count'] = texts_lower.apply(\n",
    "        lambda x: sum(1 for w in DISGUST_WORDS if re.search(rf'\\b{w}\\b', x))\n",
    "    )\n",
    "    \n",
    "    # Binary indicator\n",
    "    feats['has_disgust_word'] = (feats['disgust_word_count'] > 0).astype(int)\n",
    "    \n",
    "    # Capped intensity to prevent outliers\n",
    "    feats['disgust_intensity'] = feats['disgust_word_count'].clip(upper=3)\n",
    "    \n",
    "    # Bodily reaction subset\n",
    "    bodily = ['sick', 'vomit', 'puke', 'smell', 'stink', 'gag', 'nauseous']\n",
    "    feats['bodily_reference'] = texts_lower.apply(\n",
    "        lambda x: sum(1 for w in bodily if re.search(rf'\\b{w}\\b', x))\n",
    "    )\n",
    "    \n",
    "    return feats\n",
    "\n",
    "X_train_disgust = extract_disgust_features(train_df)\n",
    "X_test_disgust = extract_disgust_features(test_df)\n",
    "\n",
    "print(f\"✓ Extracted disgust features\")\n",
    "print(f\"  Training samples with disgust words: {X_train_disgust['has_disgust_word'].sum()}\")\n",
    "print(f\"  Average disgust words per text: {X_train_disgust['disgust_word_count'].mean():.2f}\")\n",
    "\n",
    "\n",
    "## PMI-Based Emotion Lexicons ##\n",
    "\n",
    "def learn_pmi_lexicons(df, labels, top_k=300):\n",
    "    \"\"\"\n",
    "    Learn emotion-specific word lexicons using Pointwise Mutual Information (PMI).\n",
    "    \n",
    "    1. PMI measures the association between a word and an emotion class:\n",
    "    2. PMI(word, emotion) = log(P(word|emotion) / P(word))\n",
    "\n",
    "    --> High PMI indicates the word is strongly associated with that emotion.\n",
    "    \"\"\"\n",
    "    class_counts = defaultdict(Counter)\n",
    "    total_counts = Counter()\n",
    "    \n",
    "    # Count word occurrences per class\n",
    "    for text, label_idx in zip(df['clean_text'], labels):\n",
    "        words = set(text.split())  # Use set to count document frequency\n",
    "        label = target_names[label_idx]\n",
    "        class_counts[label].update(words)\n",
    "        total_counts.update(words)\n",
    "    \n",
    "    # Calculate PMI for each word-emotion pair\n",
    "    label_counts = pd.Series(labels).value_counts()\n",
    "    total_docs = len(labels)\n",
    "    learned_lexicons = {}\n",
    "    \n",
    "    for label in target_names:\n",
    "        word_scores = {}\n",
    "        label_idx = le.transform([label])[0]\n",
    "        \n",
    "        for word, count in class_counts[label].items():\n",
    "            # Require minimum frequency to avoid noise\n",
    "            if count < 5:\n",
    "                continue\n",
    "            \n",
    "            # Calculate PMI\n",
    "            p_w_given_c = count / label_counts[label_idx]\n",
    "            p_w = total_counts[word] / total_docs\n",
    "            pmi = math.log(p_w_given_c / (p_w + 1e-8) + 1e-8)\n",
    "            \n",
    "            word_scores[word] = pmi\n",
    "        \n",
    "        # Keep top-k words with highest PMI\n",
    "        top_words = sorted(word_scores.items(), key=lambda x: x[1], reverse=True)[:top_k]\n",
    "        learned_lexicons[label] = set([w for w, s in top_words])\n",
    "    \n",
    "    return learned_lexicons\n",
    "\n",
    "# Count matches against learned emotion lexicons\n",
    "def apply_pmi_features(text_series, lexicons):\n",
    "    feats = pd.DataFrame()\n",
    "    for label in target_names:\n",
    "        lex = lexicons[label]\n",
    "        feats[f'{label}_pmi'] = text_series.apply(\n",
    "            lambda x: sum(1 for w in x.split() if w in lex)\n",
    "        )\n",
    "    return feats\n",
    "\n",
    "# Learn lexicons from training data\n",
    "pmi_lexicons = learn_pmi_lexicons(train_df, y_train, top_k=300)\n",
    "\n",
    "# Apply to both train and test\n",
    "X_train_pmi = apply_pmi_features(train_df['clean_text'], pmi_lexicons)\n",
    "X_test_pmi = apply_pmi_features(test_df['clean_text'], pmi_lexicons)\n",
    "\n",
    "print(f\"✓ Learned PMI lexicons\")\n",
    "print(f\"  Lexicon sizes: {[len(pmi_lexicons[e]) for e in target_names]}\")\n",
    "print(f\"\\nExample words per emotion:\")\n",
    "for emotion in target_names:\n",
    "    sample_words = list(pmi_lexicons[emotion])[:5]\n",
    "    print(f\"  {emotion:12s}: {', '.join(sample_words)}\")\n",
    "\n",
    "\n",
    "## TF-IDF Features ##\n",
    "\n",
    "# Word-level TF-IDF (1-grams and 2-grams)\n",
    "tfidf_word = TfidfVectorizer(\n",
    "    max_features=15000,      # Keep top 15k features\n",
    "    ngram_range=(1, 2),      # Unigrams and bigrams\n",
    "    sublinear_tf=True        # Use log scaling for term frequency\n",
    ")\n",
    "X_train_word = tfidf_word.fit_transform(train_df['clean_text'])\n",
    "X_test_word = tfidf_word.transform(test_df['clean_text'])\n",
    "\n",
    "print(f\"✓ Word TF-IDF: {X_train_word.shape[1]} features\")\n",
    "\n",
    "# Character-level TF-IDF (captures misspellings and style)\n",
    "tfidf_char = TfidfVectorizer(\n",
    "    max_features=8000,\n",
    "    ngram_range=(2, 4),      # 2-4 grams\n",
    "    analyzer='char',\n",
    "    sublinear_tf=True\n",
    ")\n",
    "X_train_char = tfidf_char.fit_transform(train_df['clean_text'])\n",
    "X_test_char = tfidf_char.transform(test_df['clean_text'])\n",
    "\n",
    "print(f\"✓ Char TF-IDF: {X_train_char.shape[1]} features\")\n",
    "\n",
    "\n",
    "## Dimensionality Reduction (SVD)##\n",
    "\n",
    "# Apply Truncated SVD to word TF-IDF for dense semantic features\n",
    "svd = TruncatedSVD(n_components=120, random_state=SEED)\n",
    "X_train_svd = svd.fit_transform(X_train_word)\n",
    "X_test_svd = svd.transform(X_test_word)\n",
    "\n",
    "print(f\"✓ SVD features: {X_train_svd.shape[1]} components\")\n",
    "print(f\"  Explained variance: {svd.explained_variance_ratio_.sum():.2%}\")\n",
    "\n",
    "\n",
    "## Sentiment Features (VADER)##\n",
    "\n",
    "# VADER: positive, negative, neutral, compound scores\n",
    "sid = SentimentIntensityAnalyzer()\n",
    "\n",
    "def get_vader_features(text_series):\n",
    "\n",
    "    return pd.DataFrame([\n",
    "        list(sid.polarity_scores(str(t)).values()) \n",
    "        for t in text_series\n",
    "    ])\n",
    "\n",
    "X_train_vader = get_vader_features(train_df['text'])\n",
    "X_test_vader = get_vader_features(test_df['text'])\n",
    "\n",
    "# Standardize sentiment scores\n",
    "scaler_vader = StandardScaler()\n",
    "X_train_vader = scaler_vader.fit_transform(X_train_vader)\n",
    "X_test_vader = scaler_vader.transform(X_test_vader)\n",
    "\n",
    "\n",
    "## Feature Combination##\n",
    "\n",
    "# Combine PMI and disgust features\n",
    "X_train_mining = pd.concat([X_train_pmi, X_train_disgust], axis=1)\n",
    "X_test_mining = pd.concat([X_test_pmi, X_test_disgust], axis=1)\n",
    "\n",
    "# Scale data mining features to [0, 1]\n",
    "scaler_mining = MinMaxScaler()\n",
    "X_train_mining_scaled = scaler_mining.fit_transform(X_train_mining)\n",
    "X_test_mining_scaled = scaler_mining.transform(X_test_mining)\n",
    "\n",
    "# Stack all feature types for training and testing (sparse + dense)\n",
    "X_train_classical = hstack([\n",
    "    X_train_word,           \n",
    "    X_train_char,           \n",
    "    X_train_svd,            \n",
    "    X_train_vader,          \n",
    "    X_train_mining_scaled   \n",
    "]).tocsr()\n",
    "\n",
    "X_test_classical = hstack([\n",
    "    X_test_word,\n",
    "    X_test_char,\n",
    "    X_test_svd,\n",
    "    X_test_vader,\n",
    "    X_test_mining_scaled\n",
    "]).tocsr()\n",
    "\n",
    "print(f\"✓ Combined feature matrix: {X_train_classical.shape}\")\n",
    "print(f\"  Total features: {X_train_classical.shape[1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Model Implementation Steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "## Class Weights (Handle Imbalance) ##\n",
    "\n",
    "# Calculate custom weights with extra boost for disgust\n",
    "class_counts = pd.Series(y_train).value_counts().sort_index()\n",
    "n_samples = len(y_train)\n",
    "custom_weights = {}\n",
    "\n",
    "for i in range(NUM_CLASSES):\n",
    "    # Base balanced weight\n",
    "    base_weight = n_samples / (NUM_CLASSES * class_counts[i])\n",
    "    \n",
    "    # Extra weight for disgust (hardest class to detect)\n",
    "    if target_names[i] == 'disgust':\n",
    "        custom_weights[i] = base_weight * 2.5\n",
    "    else:\n",
    "        custom_weights[i] = base_weight\n",
    "\n",
    "print(\"Class weights:\")\n",
    "for i in range(NUM_CLASSES):\n",
    "    print(f\"  {target_names[i]:12s}: {custom_weights[i]:.2f}\")\n",
    "\n",
    "## Model Training with Calibration ##\n",
    "\n",
    "# Linear SVM (for high-dimensional text data)\n",
    "svc = LinearSVC(\n",
    "    C=0.2,                      # Regularization strength\n",
    "    class_weight=custom_weights, # Handle class imbalance\n",
    "    max_iter=3000,              # More iterations for convergence\n",
    "    dual=False,                 # Primal form faster for n_samples > n_features\n",
    "    random_state=SEED\n",
    ")\n",
    "\n",
    "# Calibrate to get probability estimates \n",
    "classical_model = CalibratedClassifierCV(svc, method='sigmoid', cv=3)\n",
    "classical_model.fit(X_train_classical, y_train)\n",
    "\n",
    "## Generate Probability Predictions ##\n",
    "\n",
    "# Cross-validation for train probabilities\n",
    "classical_train_probs = cross_val_predict(\n",
    "    classical_model, \n",
    "    X_train_classical, \n",
    "    y_train, \n",
    "    cv=3, \n",
    "    method='predict_proba',\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "# Direct prediction for test\n",
    "classical_test_probs = classical_model.predict_proba(X_test_classical)\n",
    "\n",
    "# Save for ensemble use\n",
    "np.save('./logs/classical_train_probs.npy', classical_train_probs)\n",
    "np.save('./logs/classical_test_probs.npy', classical_test_probs)\n",
    "\n",
    "# Evaluate classical model alone\n",
    "classical_preds = np.argmax(classical_train_probs, axis=1)\n",
    "classical_f1 = f1_score(y_train, classical_preds, average='macro')\n",
    "print(f\"\\nClassical Model Performance:\")\n",
    "print(f\"  Macro F1: {classical_f1:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Minimal Cleaning for Transformers ##\n",
    "# Pre-trained on noisy, real-world text (no need to over-clean)\n",
    "\n",
    "def clean_text_minimal(text):\n",
    "    text = str(text)\n",
    "    # Remove URLs (transformers don't learn from these)\n",
    "    text = re.sub(r'http\\S+|www\\S+', '', text)\n",
    "    # Remove excessive whitespace\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    return text\n",
    "\n",
    "train_df['clean_text'] = train_df['text'].apply(clean_text_minimal)\n",
    "test_df['clean_text'] = test_df['text'].apply(clean_text_minimal)\n",
    "\n",
    "## Creating Dataset ##\n",
    "\n",
    "class EmotionDataset(Dataset):\n",
    "        PyTorch Dataset for emotion classification.\n",
    "    \"\"\"\n",
    "    1. Tokenization with padding and truncation\n",
    "    2. Attention masks (which tokens are real vs padding)\n",
    "    3. Conversion to PyTorch tensors\n",
    "    \"\"\"\n",
    "    def __init__(self, texts, labels, tokenizer, max_length=128):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = str(self.texts[idx])\n",
    "        label = self.labels[idx]\n",
    "        \n",
    "        # Tokenize with special tokens\n",
    "        encoding = self.tokenizer(\n",
    "            text,\n",
    "            add_special_tokens=True,    # Add [CLS] and [SEP] tokens\n",
    "            max_length=self.max_length,  # Truncate long sequences \n",
    "            padding='max_length',        # Pad short sequences\n",
    "            truncation=True,             # Enable truncation\n",
    "            return_attention_mask=True,  # Return attention mask\n",
    "            return_tensors='pt'          # Return PyTorch tensors\n",
    "        )\n",
    "\n",
    "        return {\n",
    "            'input_ids': encoding['input_ids'].flatten(),\n",
    "            'attention_mask': encoding['attention_mask'].flatten(),\n",
    "            'labels': torch.tensor(label, dtype=torch.long)\n",
    "        }\n",
    "\n",
    "\n",
    "## Load RoBERTa Model and Tokenizer ##\n",
    "\n",
    "MODEL_NAME = 'roberta-base'\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "\n",
    "## Class Weights for Imbalanced Data ##\n",
    "\n",
    "class_counts = pd.Series(y_train).value_counts().sort_index()\n",
    "n_samples = len(y_train)\n",
    "\n",
    "# Balanced weights \n",
    "class_weights = []\n",
    "for i in range(NUM_CLASSES):\n",
    "    base_weight = n_samples / (NUM_CLASSES * class_counts[i])\n",
    "    if target_names[i] == 'disgust':\n",
    "        weight = base_weight * 2.0  # Extra boost for disgust (hardest class)\n",
    "    else:\n",
    "        weight = base_weight\n",
    "    class_weights.append(weight)\n",
    "\n",
    "class_weights = torch.FloatTensor(class_weights).to(device)\n",
    "\n",
    "print(\"Class weights:\")\n",
    "for i, emotion in enumerate(target_names):\n",
    "    print(f\"  {emotion:12s}: {class_weights[i]:.3f}\")\n",
    "\n",
    "## Training Function ##\n",
    "\n",
    "def train_epoch(model, data_loader, optimizer, scheduler, device, class_weights):\n",
    "    model.train()   # Set to training mode\n",
    "    losses = []\n",
    "    correct_predictions = 0\n",
    "\n",
    "    progress_bar = tqdm(data_loader, desc='Training')\n",
    "\n",
    "    for batch in progress_bar:\n",
    "        # Move batch to device\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['labels'].to(device)\n",
    "        \n",
    "        # Zero gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            labels=labels\n",
    "        )\n",
    "\n",
    "        # Apply class weights to loss\n",
    "        loss = outputs.loss\n",
    "\n",
    "        # Get predictions\n",
    "        preds = torch.argmax(outputs.logits, dim=1)\n",
    "        correct_predictions += torch.sum(preds == labels)\n",
    "\n",
    "        losses.append(loss.item())\n",
    "\n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "\n",
    "        # Gradient clipping (prevent exploding gradients)\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "\n",
    "        # Update weights\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "\n",
    "        progress_bar.set_postfix({'loss': np.mean(losses), 'acc': correct_predictions.double().item() / len(data_loader.dataset)})\n",
    "\n",
    "    return correct_predictions.double() / len(data_loader.dataset), np.mean(losses)\n",
    "\n",
    "## Evaluation Function ##\n",
    "\n",
    "def eval_model(model, data_loader, device):\n",
    "    model.eval()    # Set to evaluation mode\n",
    "    losses = []\n",
    "    predictions = []\n",
    "    true_labels = []\n",
    "\n",
    "    with torch.no_grad():      # Disable gradient computation\n",
    "        progress_bar = tqdm(data_loader, desc='Evaluating')\n",
    "        for batch in progress_bar:\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels = batch['labels'].to(device)\n",
    "\n",
    "            outputs = model(\n",
    "                input_ids=input_ids,\n",
    "                attention_mask=attention_mask,\n",
    "                labels=labels\n",
    "            )\n",
    "\n",
    "            loss = outputs.loss\n",
    "            preds = torch.argmax(outputs.logits, dim=1)\n",
    "\n",
    "            predictions.extend(preds.cpu().numpy())\n",
    "            true_labels.extend(labels.cpu().numpy())\n",
    "            losses.append(loss.item())\n",
    "\n",
    "    return predictions, true_labels, np.mean(losses)\n",
    "\n",
    "## Prediction with Probabilities ##\n",
    "\n",
    "def get_predictions_with_proba(model, data_loader, device):\n",
    "    \"\"\"\n",
    "    Get predictions and probability distributions.\n",
    "        1. Ensemble methods (combining multiple models)\n",
    "        2. Confidence analysis\n",
    "        3. Threshold optimization\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    predictions = []\n",
    "    probabilities = []\n",
    "    true_labels = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(data_loader, desc='Predicting'):\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels = batch['labels'].to(device)\n",
    "\n",
    "            outputs = model(\n",
    "                input_ids=input_ids,\n",
    "                attention_mask=attention_mask\n",
    "            )\n",
    "\n",
    "            # Convert logits to probabilities\n",
    "            probs = torch.softmax(outputs.logits, dim=1)\n",
    "            preds = torch.argmax(probs, dim=1)\n",
    "\n",
    "            probabilities.extend(probs.cpu().numpy())\n",
    "            predictions.extend(preds.cpu().numpy())\n",
    "            true_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "    return np.array(probabilities), np.array(predictions), np.array(true_labels)\n",
    "\n",
    "## K-Fold Cross- Validation Training ##\n",
    "\n",
    "# Training configuration\n",
    "N_FOLDS = 3\n",
    "EPOCHS = 3  \n",
    "BATCH_SIZE = 16 \n",
    "LEARNING_RATE = 2e-5    # Standard for fine-tuning\n",
    "\n",
    "# Storage for final predictions\n",
    "all_train_preds = np.zeros((len(train_df), NUM_CLASSES))\n",
    "all_test_preds = []\n",
    "\n",
    "# K-Fold splitter\n",
    "kfold = StratifiedKFold(n_splits=N_FOLDS, shuffle=True, random_state=SEED)\n",
    "\n",
    "# Training Loop\n",
    "for fold, (train_idx, val_idx) in enumerate(kfold.split(train_df, y_train)):\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"FOLD {fold + 1}/{N_FOLDS}\")\n",
    "    print(f\"{'='*80}\")\n",
    "\n",
    "    # Prepare datasets for each fold\n",
    "    train_texts = train_df.iloc[train_idx]['clean_text'].values\n",
    "    train_labels = y_train[train_idx]\n",
    "    val_texts = train_df.iloc[val_idx]['clean_text'].values\n",
    "    val_labels = y_train[val_idx]\n",
    "\n",
    "    train_dataset = EmotionDataset(train_texts, train_labels, tokenizer)\n",
    "    val_dataset = EmotionDataset(val_texts, val_labels, tokenizer)\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "    # Load FRESH model for each fold\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(\n",
    "        MODEL_NAME,\n",
    "        num_labels=NUM_CLASSES\n",
    "    ).to(device)\n",
    "\n",
    "    # Optimizer and scheduler\n",
    "    optimizer = AdamW(model.parameters(), lr=LEARNING_RATE)\n",
    "    total_steps = len(train_loader) * EPOCHS\n",
    "    scheduler = get_linear_schedule_with_warmup(\n",
    "        optimizer,\n",
    "        num_warmup_steps=total_steps // 10, # 10% warmup\n",
    "        num_training_steps=total_steps\n",
    "    )\n",
    "\n",
    "    # Training loop for each fold\n",
    "    best_val_f1 = 0\n",
    "    for epoch in range(EPOCHS):\n",
    "        print(f\"\\nEpoch {epoch + 1}/{EPOCHS}\")\n",
    "\n",
    "        # Training\n",
    "        train_acc, train_loss = train_epoch(\n",
    "            model, train_loader, optimizer, scheduler, device, class_weights\n",
    "        )\n",
    "        print(f\"Train loss: {train_loss:.4f}, Train acc: {train_acc:.4f}\")\n",
    "\n",
    "        # Validation\n",
    "        val_preds, val_labels_actual, val_loss = eval_model(model, val_loader, device)\n",
    "        val_f1 = f1_score(val_labels_actual, val_preds, average='macro')\n",
    "        print(f\"Val loss: {val_loss:.4f}, Val F1: {val_f1:.4f}\")\n",
    "\n",
    "        # Track best model\n",
    "        if val_f1 > best_val_f1:\n",
    "            best_val_f1 = val_f1\n",
    "            print(f\"Best F1: {val_f1:.4f}\")\n",
    "\n",
    "    # Get predictions on validation fold\n",
    "    val_preds_proba, _, _ = get_predictions_with_proba(model, val_loader, device)\n",
    "    all_train_preds[val_idx] = val_preds_proba\n",
    "\n",
    "    # Get predictions on test set\n",
    "    test_dataset = EmotionDataset(test_df['clean_text'].values, np.zeros(len(test_df)), tokenizer)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "    test_preds_proba, _, _ = get_predictions_with_proba(model, test_loader, device)\n",
    "    all_test_preds.append(test_preds_proba)\n",
    "\n",
    "    print(f\"\\nFold {fold + 1} complete. Best Val F1: {best_val_f1:.4f}\")\n",
    "\n",
    "# Average test predictions across folds\n",
    "roberta_test_probs = np.mean(all_test_preds, axis=0)\n",
    "roberta_train_probs = all_train_preds\n",
    "\n",
    "roberta_preds = np.argmax(train_probs_roberta, axis=1)\n",
    "roberta_f1 = f1_score(y_train, train_preds_roberta, average='macro')\n",
    "print(\"RoBERTa Model Marco F1: {roberta_f1:.4f}\")\n",
    "\n",
    "print(f\"RoBERTa Training F1: {train_f1_roberta:.4f}\")\n",
    "# Save probabilities for potential ensemble\n",
    "np.save('./logs/roberta_train_probs.npy', roberta_train_probs)\n",
    "np.save('./logs/roberta_test_probs.npy', roberta_test_probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Ensemble Model ###\n",
    "\n",
    "## Analyze Model Strengths ##\n",
    "for i, emotion in enumerate(target_names):\n",
    "    mask = (y_train == i)\n",
    "    if mask.sum() > 0:\n",
    "        f1_roberta = f1_score(y_train[mask], roberta_preds[mask], labels=[i], average='macro')\n",
    "        f1_classical = f1_score(y_train[mask], classical_preds[mask], labels=[i], average='macro')\n",
    "        better = \"RoBERTa\" if f1_roberta > f1_classical else \"Classical\"\n",
    "        print(f\"{emotion:<12} {f1_roberta:8.4f} {f1_classical:10.4f} {better:>15}\")\n",
    "\n",
    "## Optimize Ensemble Weights ##\n",
    "\n",
    "best_f1 = 0\n",
    "best_weights = None\n",
    "best_preds = None\n",
    "\n",
    "print(f\"\\n{'RoBERTa':>8s} {'Classical':>10s} {'Macro F1':>9s} {'Disgust F1':>11s} {'Anger F1':>9s}\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "# Grid search over different weight combinations\n",
    "for roberta_weight in [0.50, 0.55, 0.60, 0.65, 0.70, 0.75]:\n",
    "    classical_weight = 1.0 - roberta_weight\n",
    "    \n",
    "    # Weighted average of probabilities\n",
    "    ensemble_probs = (roberta_weight * roberta_train_probs) + \\\n",
    "                     (classical_weight * classical_train_probs)\n",
    "    ensemble_preds = np.argmax(ensemble_probs, axis=1)\n",
    "    \n",
    "    # Calculate metrics\n",
    "    f1_macro = f1_score(y_train, ensemble_preds, average='macro')\n",
    "    f1_disgust = f1_score(y_train == disgust_idx, ensemble_preds == disgust_idx)\n",
    "    f1_anger = f1_score(y_train == anger_idx, ensemble_preds == anger_idx)\n",
    "    \n",
    "    print(f\"{roberta_weight:8.2f} {classical_weight:10.2f} {f1_macro:9.4f} \"\n",
    "          f\"{f1_disgust:11.4f} {f1_anger:9.4f}\")\n",
    "    \n",
    "    if f1_macro > best_f1:\n",
    "        best_f1 = f1_macro\n",
    "        best_weights = (roberta_weight, classical_weight)\n",
    "        best_preds = ensemble_preds\n",
    "\n",
    "print(f\"\\nOptimal weights: RoBERTa={best_weights[0]:.2f}, Classical={best_weights[1]:.2f}\")\n",
    "print(f\"\\nTraining F1: {best_f1:.4f}\")\n",
    "\n",
    "\n",
    "## Final Ensemble Predictions ##\n",
    "\n",
    "# Apply best weights to create final ensemble\n",
    "ensemble_train_probs = (best_weights[0] * roberta_train_probs) + \\\n",
    "                       (best_weights[1] * classical_train_probs)\n",
    "ensemble_test_probs = (best_weights[0] * roberta_test_probs) + \\\n",
    "                      (best_weights[1] * classical_test_probs)\n",
    "\n",
    "preds_train = np.argmax(ensemble_train_probs, axis=1)\n",
    "preds_test = np.argmax(ensemble_test_probs, axis=1)\n",
    "\n",
    "train_f1 = f1_score(y_train, preds_train, average='macro')\n",
    "print(f\"\\nEnsemble Training F1: {train_f1:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DM2025-Lab2-Exercise",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
